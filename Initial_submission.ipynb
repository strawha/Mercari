{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "from collections import Counter\n",
    "from stop_words import get_stop_words\n",
    "import re\n",
    "from tqdm.notebook import tqdm\n",
    "import itertools\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy.sparse import hstack\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import RidgeCV\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('/kaggle/input/mercari/train.tsv', sep='\\t')\n",
    "test = pd.read_csv('/kaggle/input/mercari/test.tsv' , sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_categories(category):\n",
    "    try:\n",
    "        main_cat1,sub_cat1,sub_cat2 = category.split(\"/\")\n",
    "        return main_cat1,sub_cat1,sub_cat2\n",
    "    except:\n",
    "        return 'No Category','No Category','No Category'\n",
    "    \n",
    "def remove_punct(token):\n",
    "    x = [i for i in token if i.isalnum()]\n",
    "    return x\n",
    "\n",
    "def remove_stopwords(token):\n",
    "    x = [i for i in token if not i in stopwords]\n",
    "    return x\n",
    "\n",
    "def stemmer(token):\n",
    "    porter = PorterStemmer()\n",
    "    x = [porter.stem(i) for i in token]\n",
    "    return x\n",
    "\n",
    "def lemmatizer(token):\n",
    "    lemmatizer = WordNetLemmatizer() \n",
    "    x = [lemmatizer.lemmatize(i) for i in token]\n",
    "    return x\n",
    "\n",
    "def join(token):\n",
    "    x = [\" \".join(i for i in token)]\n",
    "    return x[0]\n",
    "\n",
    "def encode(train,test):\n",
    "    vectorizer = CountVectorizer()\n",
    "\n",
    "    vectorizer = vectorizer.fit(train['brand_name'].values)\n",
    "    brand = vectorizer.transform(test['brand_name'].values)\n",
    "    \n",
    "    vectorizer = vectorizer.fit(train['category_name'].values)\n",
    "    category = vectorizer.transform(test['category_name'].values)\n",
    "    \n",
    "    vectorizer = vectorizer.fit(train['main_cat'].values)\n",
    "    maincat = vectorizer.transform(test['main_cat'].values)\n",
    "    \n",
    "    vectorizer = vectorizer.fit(train['sub_cat1'].values)\n",
    "    subcat1 = vectorizer.transform(test['sub_cat1'].values)\n",
    "    \n",
    "    vectorizer = vectorizer.fit(train['sub_cat2'].values)\n",
    "    subcat2 = vectorizer.transform(test['sub_cat2'].values)\n",
    "    \n",
    "    vectorizer = vectorizer.fit(train['tokenized_name'].values)\n",
    "    name = vectorizer.transform(test['tokenized_name'].values)\n",
    "    \n",
    "    vectorizer = vectorizer.fit(train['tokenized_description'].values)\n",
    "    description = vectorizer.transform(test['tokenized_description'].values)\n",
    "\n",
    "    return brand,category,maincat,subcat1,subcat2,name,description\n",
    "\n",
    "def encode1(train,test,column,feature,ranges):\n",
    "    \n",
    "    vectorizer = TfidfVectorizer(ngram_range=ranges, max_features=feature).fit(train[column])\n",
    "    transformed_text = vectorizer.transform(test[column])\n",
    "    return transformed_text\n",
    "\n",
    "def encode2(train,test,column,feature,ranges):\n",
    "   \n",
    "    vectorizer = CountVectorizer(ngram_range=ranges, max_features=feature).fit(train[column])\n",
    "    transformed_text = vectorizer.transform(test[column])\n",
    "    return transformed_text\n",
    "    \n",
    "def decontracted(phrase):\n",
    "    phrase = re.sub(r\"won't\", \"will not\", phrase)\n",
    "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
    "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
    "    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
    "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
    "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
    "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
    "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
    "    return phrase\n",
    "    \n",
    "def dummies(df):\n",
    "    df['item_condition_id'] = df[\"item_condition_id\"].astype(\"category\")\n",
    "    df['shipping'] = df[\"shipping\"].astype(\"category\")\n",
    "    item_id_shipping = csr_matrix(pd.get_dummies(df[['item_condition_id', 'shipping']],sparse=True).values)\n",
    "    return item_id_shipping\n",
    "\n",
    "def original_price(value):\n",
    "    return math.exp(value) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['main_cat'],train['sub_cat1'],train['sub_cat2'] = zip(*train['category_name'].apply(lambda x: split_categories(x)))\n",
    "test['main_cat'],test['sub_cat1'],test['sub_cat2'] = zip(*test['category_name'].apply(lambda x: split_categories(x)))\n",
    "train['log_price'] = np.log1p(train['price'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counter = Counter(train['item_description'])\n",
    "most_common_words = word_counter.most_common(500)\n",
    "\n",
    "stopwords = get_stop_words('en')\n",
    "stopwords.extend(['rm'])  \n",
    "#it is mentioned that information given to us is in a formatted way and it doesn't show text given in \n",
    "#dollar terms instead we get rm in it's place so it is added as aditional stop words since it occurences have \n",
    "#no effect on our prices\n",
    "\n",
    "#here is the link to original mercari dataset where it is explained\n",
    "#https://www.kaggle.com/c/mercari-price-suggestion-challenge/data\n",
    "\n",
    "wordcloud = WordCloud(stopwords=stopwords,background_color='white').generate(str(most_common_words))\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(wordcloud,interpolation='bilinear')\n",
    "plt.title('Word cloud of item description\\n',fontsize=15)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['description_wc']=[len(str(i).split()) for i in train['item_description']]\n",
    "test['description_wc']=[len(str(i).split()) for i in test['item_description']]\n",
    "train['name_wc']=[len(str(i).split()) for i in train['name']]\n",
    "test['name_wc']=[len(str(i).split()) for i in test['name']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['brand_name'] = train['brand_name'].fillna('Not Known')\n",
    "train['item_description'] = train['item_description'].fillna('No Description Yet')\n",
    "train['category_name'] = train['category_name'].fillna('Not Category')\n",
    "test['category_name'] = test['category_name'].fillna('No Category')\n",
    "test['brand_name'] = test['brand_name'].fillna('Not Known')\n",
    "test['item_description'] = test['item_description'].fillna('No Description Yet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train['item_description'] = train['item_description'].apply(decontracted)\n",
    "#test['item_description'] = test['item_description'].astype(str).apply(decontracted)\n",
    "#train['name'] = train['name'].apply(decontracted)\n",
    "#test['name']= test['name'].apply(decontracted)\n",
    "\n",
    "#got better result without using them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['tokenized_name'] = train.apply(lambda x:word_tokenize(str(x['name'])),axis = 1)\n",
    "test['tokenized_name'] = test.apply(lambda x:word_tokenize(str(x['name'])),axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['tokenized_name'] = train['tokenized_name'].apply(remove_punct)\n",
    "test['tokenized_name'] = test['tokenized_name'].apply(remove_punct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['tokenized_name'] = train['tokenized_name'].apply(remove_stopwords)\n",
    "test['tokenized_name'] = test['tokenized_name'].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['tokenized_name'] = train['tokenized_name'].apply(join)\n",
    "test['tokenized_name'] = test['tokenized_name'].apply(join)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['tokenized_description'] = train.apply(lambda x:word_tokenize(str(x['item_description'])),axis = 1)\n",
    "test['tokenized_description'] = test.apply(lambda x:word_tokenize(str(x['item_description'])),axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['tokenized_description'] = train['tokenized_description'].apply(remove_punct)\n",
    "test['tokenized_description] = test['tokenized_description'].apply(remove_punct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['tokenized_description'] = train['tokenized_description'].apply(remove_stopwords)\n",
    "test['tokenized_description'] = test['tokenized_description'].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['tokenized_description'] = train['tokenized_description'].apply(join)\n",
    "test['tokenized_description'] = test['tokenized_description'].apply(join)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brand_train,cat_train,maincat_train,subcat1_train,subcat2_train,name_train,description_train = encode(train,train)\n",
    "brand_test,cat_test,maincat_test,subcat1_test,subcat2_test,name_test,description_test  = encode(train,test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_id_shipping_train = dummies(train)\n",
    "item_id_shipping_test = dummies(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_set = hstack((brand_train,cat_train,maincat_train,subcat1_train,subcat2_train,item_id_shipping_train)).tocsr()\n",
    "x_test_set = hstack((brand_test,cat_test,maincat_test,subcat1_test,subcat2_test,item_id_shipping_test)).tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tfidf = encode1(train,train,'tokenized_name',100000,(1,2))\n",
    "X_test_tfidf = encode1(train,test,'tokenized_name',100000,(1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tfidf1 = encode1(train,train,'tokenized_description',100000,(1,2))\n",
    "X_test_tfidf1 = encode1(train,test,'tokenized_description',100000,(1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = hstack((X_train_tfidf,X_train_tfidf1,x_train_set,train['name_wc'].values.reshape(-1,1),train['description_wc'].values.reshape(-1,1))).tocsr()\n",
    "x_test = hstack((X_test_tfidf,X_test_tfidf1,x_test_set,test['name_wc'].values.reshape(-1,1),test['description_wc'].values.reshape(-1,1))).tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train['log_price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'alpha':[0.0001,0.001,0.01,0.1,1.0,2.0,4.0,5.0,6.0]}\n",
    "model_ridge = Ridge(\n",
    "    solver='auto', fit_intercept=True,\n",
    "    max_iter=100, normalize=False, tol=0.05, random_state = 1,\n",
    ")\n",
    "r_model = RandomizedSearchCV(model_ridge,params,scoring='neg_mean_squared_error',cv=2,verbose=2,n_jobs=-1,return_train_score=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_model.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_model.best_params_   #get alpha value to use in next step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_modelCV = RidgeCV(\n",
    "    fit_intercept=True, alphas=[4.0],\n",
    "    normalize=False, cv = 2, scoring='neg_mean_squared_error',\n",
    ")\n",
    "ridge_modelCV.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred=ridge_modelCV.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(pred)\n",
    "df[0] = df[0].apply(original_price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_id = test['id']\n",
    "result = pd.concat([test_id,df],axis = 1)\n",
    "result.set_index('id', inplace=True)\n",
    "result.rename(columns = {0:'price'}, inplace = True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.to_csv('/kaggle/working/submission_kaggle.csv') #now download submission_kaggle.csv and submit it"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
